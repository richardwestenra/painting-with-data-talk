<!doctype html>
<html itemscope itemtype="http://schema.org/Article">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>The Good Life</title>
		<meta name="description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?" />

		<!-- Schema.org markup for Google+ -->
		<meta itemprop="name" content="The Good Life">
		<meta itemprop="description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?">
		<meta itemprop="image" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png">

		<!-- Twitter Card data -->
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@richardwestenra">
		<meta name="twitter:title" content="The Good Life">
		<meta name="twitter:description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?">
		<meta name="twitter:creator" content="@richardwestenra">
		<meta name="twitter:image:src" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png">

		<!-- Open Graph data -->
		<meta property="og:title" content="The Good Life" />
		<meta property="og:type" content="article" />
		<meta property="og:url" content="https://www.richardwestenra.com/good-life-talk/" />
		<meta property="og:image" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png" />
		<meta property="og:description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?" />
		<meta property="og:site_name" content="Richard Westenra" />
		<meta property="article:published_time" content="2017-10-26T19:00:00+01:00" />

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">
		<link rel="stylesheet" href="css/theme/custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<span class="twitter-handle">@richardwestenra</span>

		<div class="reveal">
			<div class="slides">
<section data-background-color="white">
	<h1 class="title"><span>The</span> <b>Good Life</b></h1>
	<aside class="notes">
		<p>Hi, my name is Richard, and I‚Äôm here today to talk to you about what is best in life.</p>
		<p>I originally wrote this talk for an audience of front-end web developers, but it applies broadly to anyone who works in tech or engineering disciplines, or anyone who cares about ethics in tech.</p>
		<p>It's called The Good Life, or as I like to call it: Actually, it's about ethics in software engineering.</p>
	</aside>
</section>
<section data-background="img/fedora.jpg" data-background-size="contain">
	<h3><b class="actually">Actually,</b> it's about ethics in software engineering</h3>
	<aside class="notes">
		<p>About my background: I started out at university studying mechanical engineering, which I hated. I quickly switched to philosophy, which I loved. Now, ten years later, my official job title is front end engineer. Go figure.</p>
		<p>So I'm a front-end engineer these days, but I usually refer to myself as a developer. I've always felt a bit uncomfortable with the title of engineer, though I couldn't really put my finger on why. Maybe it felt a bit like I was claiming some sort of prestige that I hadn't earned?</p>
		<p>I didn't complete my engineering degree or study computer science: I'm mostly self-taught. I like to think I‚Äôm pretty good at my job, but I still feel like I'm hacking stuff together a lot of the time.</p>
	</aside>
</section>
<section data-background="img/webmaster.jpg" data-background-size="contain">
	<aside class="notes">
		<p>Back in the day, many of us who build websites used to call ourselves webmasters. Which I still think is an amazing job title, btw. I'm kinda sad that I didn't start my career quite early enough to have a webmaster role in the job history section of my CV. It's a bit like Social Justice Warrior, like, people use it as an insult but frankly I think it sounds badass.</p>
		<p>Anyway so we were webmasters - and webmistresses...</p>
</section>
<section data-background="img/webmistress.jpg" data-background-size="contain">
	<aside class="notes">
		<p>And then that title fell out of fashion and we became web developers, and now many of us have the very professional-sounding title of front-end engineers.</p>
		<p>I don‚Äôt think that's a bad thing, or that we should stop calling ourselves engineers - call yourself whatever you want, you can be a techno-viking or an agile pirate-ninja riding a full-stack glitter-unicorn if it makes you feel better.</p>
	</aside>
</section>
<section data-background="img/spacejam.png" data-background-size="cover">
	<h3 class="hashtag">#ObligatorySpaceJam</h3>
	<aside class="notes">
		<p>But we used to be working on dinky little websites that were kind of thrown together, and now we're working on massive, professional operations that form the financial backbone of multi-billion dollar corporations. We've come a long, long way from being webmasters and webmistresses.</p>
		<p>But a lot of us are self-taught and didn't go through any standardised training process. And don't get me wrong, this is great, it helps ease barriers to entry. But as the industry gets more professional, it might be worth thinking about other trappings of professionalism. If we're going to call ourselves engineers, then there are ethical duties and codes of responsibility that go along with that title.</p>
	</aside>
</section>
<section data-background="img/forth.jpg" data-background-size="cover" data-background-position="right 35%">
	<aside class="notes">
		<p>A hundred years ago, civil engineering was in a similar situation to how the tech industry is now. As the industrial revolution receded behind them, engineers found new ways to use all the fancy new technologies they had developed. They grew more sophisticated in their approach, and their projects ballooned in scale and complexity.</p>
		<p>But as these projects became more ambitious, there was an accompanying problem: A rise in major engineering disasters. The turn of the twentieth century saw a wave of epic structural failures, including some massive bridge collapses,</p>
	</aside>
</section>
	<section data-background="img/molasses.jpg" data-background-size="contain">
	<h4 class="hashtag">#StickySituation</h4>
	<h4 class="hashtag">#TooSoon</h4>
	<aside class="notes">
		<p>&hellip;and also the Great Boston Molasses Flood, which you can see here.</p>
		<p>Which, if I had to name my favourite disaster of all time, this would have to be it, just for the mental image of a tsunami of liquid sugar 50 feet high, travelling 35 miles an hour, consuming everything in its path. It's terrifying, but also, kinda delicious?</p>
		<p>Anyway these disasters had a profound effect on the way the public saw engineering, and forced engineers to confront their shortcomings.</p>
		<p>As a result, they began to regulate themselves more intensely, and established standardised industry codes of ethics.</p>
	</aside>
</section>
<section data-background="img/conan.gif" data-background-size="cover">
	<aside class="notes">
		<p>So, what is Ethics?</p>
		<p>Ethics is a branch of philosophy devoted to answering questions about what is best in life. Questions like these:</p>
	</aside>
</section>
<section data-background="img/conan.gif" data-background-size="cover">
	<dl class="ethics">
		<dt>‚ÄúWhat is best?‚Äù</dt>
		<dd class="fragment">Both spaces AND tabs, on alternating lines</dd>
		<dt>‚ÄúWhat is the good life?‚Äù</dt>
		<dd class="fragment">When the client is banned from feature requests</dd>
		<dt>‚ÄúHow should I live?‚Äù</dt>
		<!-- <dd class="fragment">Only support one browser</dd> -->
		<dd class="fragment">Outsource your job to China and spend all day on Reddit</dd>
		<dt>‚ÄúHow should I behave towards other people?‚Äù</dt>
		<dd class="fragment">Interrupt them when they have their headphones on</dd>
		<dt>‚ÄúWhat is the purpose of life?‚Äù</dt>
		<dd class="fragment">Replacing everything with JavaScript</dd>
	</dl>
	<aside class="notes">
		<p>And I know what you're thinking, I can see the cogs in your software developer minds turning over. You're thinking "That's EASY!"</p>
		<p>DOWN x 4</p>
		<p>...You're all monsters. Though I'm with you on that client thing, I'd like to go one step further and ban all clients, but that's a different topic for another day.</p>
	</aside>
</section>
<section data-background="img/chidi.jpg" data-background-size="cover">
	<aside class="notes">
		<p>Philosophers like to do things called "thought experiments", which are like real experiments but even better, because you never need to get out of your armchair.</p>
		<p>One of the most famous is the trolley problem. I'm sure many of you are already familiar with it, but for those who aren't, here's the deal.</p>
	</aside>
</section>
<section data-background="img/trolley-problem.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>There is a runaway trolley barreling down the railway tracks. Ahead, on the tracks, there are five people tied up and unable to move. The trolley is headed straight for them. You are standing some distance off in the train yard, next to a lever. If you pull this lever, the trolley will switch to a different set of tracks. However, you notice that there is one person on the side track. You have two options:</p>
		<p>Do nothing, and the trolley kills the five people on the main track.</p>
		<p>Pull the lever, diverting the trolley onto the side track where it will kill one person.</p>
		<p>Which is the most ethical choice? Quick show of hands, no wrong answers.</p>
	</aside>
</section>
<section data-background="img/fat-man.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>Now, imagine instead of a switch, you‚Äôre standing on a bridge over the tracks, next to an extremely large man. The trolley is coming, and the only way you can stop it is to push the large man onto the tracks. He‚Äôs the only one big enough to slow down the trolley.</p>
		<p>He‚Äôs looking you right in the eyes, he can see what you‚Äôre thinking, and he‚Äôs terrified, begging you not to do it. What do you do?</p>
	</aside>
</section>
<section>
	<p><div class="emoji">üõ†</div> 9 out of 10 people would<br> throw the switch,</p>
	<p><div class="emoji emoji--gap">üôÖ‚Äç‚ôÇÔ∏è</div> but only 1 in 10 people would<br> push the large man.</p>
	<aside class="notes">
		<p>The trolley problem has been the subject of many surveys, which tend to find that approximately 9 out of 10 respondents would throw the switch to kill the one and save the five.</p>
		<p>However in the large man scenario, the situation reverses, and only 1 in 10 people would push him onto the tracks.</p>
		<p>Incidentally, a 2009 survey of professional philosophers found that only 68% of them would throw the switch, 8% would not switch, and the remaining 24% had another view or could not answer. So if you‚Äôre ever tied to a train track by a cartoon villain, you‚Äôd better hope that the person by the switch isn‚Äôt a moral philosopher.</p>
	</aside>
</section>
<section data-background="img/brain.gif" data-background-size="cover">
	<aside class="notes">
		<p>So why the difference in the two outcomes?</p>
		<p>One theory is that it's because two different parts of your brain are fighting with each other.</p>
		<p>Some researchers looked at people‚Äôs brains using fMRI machines, and demonstrated that "personal" dilemmas (like pushing a man off a footbridge) engage brain regions associated with emotion, whereas "impersonal" dilemmas (like diverting the trolley by flipping a switch) engaged regions associated with controlled reasoning.</p>
		<p>And these different brain processes essentially compete with each other whenever you have to make a tough moral decision.</p>
	</aside>
</section>
<section data-background="img/mvr.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>Basically, inside your brain, you‚Äôve got a monkey and a robot fighting over the controls. Every time you have to make a moral decision, they duke it out.</p>
		<p>The monkey understands something simple like pushing someone off a bridge, and it's horrified. But it doesn't understand something complex like a mechanical switch, so in that situation, the gut response is reduced, and we're able to throw the lever without feeling such a crushing sense of moral horror.</p>
		<p>Now some people have a stronger monkey, and some people have a stronger robot, and that's great, because both are useful in different situations.</p>
		<p>But this is tricky for we programmers, because we work on complex problems, which might make it trickier for our monkey brains to trigger some kinds of moral responses.</p>
	</aside>
</section>
<section data-background="img/self-driving-cars.jpg" data-background-size="cover">
	<aside class="notes">
		<p>By the way, if you think it's hard for programmers to experience the full range of ethical response, then spare a thought for autonomous vehicles.</p>
		<p>Self-driving cars don‚Äôt have meat brains, and you can‚Äôt make a perfectly ethical algorithm: You can only make it as good as the humans who programmed it, and we can‚Äôt even agree on whether to use tabs or spaces.</p>
	</aside>
</section>
<!-- <section data-background="img/fibonacci.png" data-background-size="contain" data-background-color="white">
	<aside class="notes">
		Incidentally, the correct answer is increasing the spacing for each successive indent according to the Fibbonaci sequence, but I digress.
	</aside>
</section> -->
<section data-background="img/lidar.jpg" data-background-size="cover">
	<aside class="notes">
		<p>There are also some really tricky problems here that self-driving cars will face. Like, we'd prefer a self-driving car to swerve into a pile of trash rather than hit someone, just like a person might. But computers can make these kinds of decisions much quicker than we can, so if we decide in advance what we want them to do, they'll follow our instructions.</p>
		<p>So, we'd probably want to program our car to hit a single adult rather than bus-load of school children, right? But what if the adult is a Nobel-prize winning cancer researcher? What if the adult is driving the car? Would we want the car to sacrifice its driver? And would you choose to buy a self-driving car that's designed to sacrifice your life to save others?</p>
	</aside>
</section>
<section data-background="img/moralmachine.png" data-background-size="cover" data-background-position="center top">
	<p class="hashtag" style="margin: 0">moralmachine.mit.edu</p>
	<aside class="notes">
		<p>Some researchers at MIT came up with a nice solution for this: They built an app to mine data on people‚Äôs answers to different trolley problems, so they can use it to help them decide how autonomous cars should behave in different scenarios.</p>
		<p>The website is called Moral Machine, and you can go there right now and start judging scenarios, like this one, where you have to choose between a male athlete driving a car, and a jaywalking baby. On the one hand, the baby probably doesn't know not to cross on a red signal, but on the other hand, it might grow up to be Hitler, you know? So it's a tough call.</p>
		<p>Anyway, Moral Machine is cool, but it doesn't help us, because we can't outsource all our ethical decision-making to the internet. We're just individuals working on our laptops, and we have these ridiculous meat brains, and we have to make our own decisions about things like whether to kill baby Hitler.</p>
		<p>So of course, we sometimes make the wrong call.</p>
	</aside>
</section>
<!-- 
<section>
	<aside class="notes">
		https://www.utilitarian.net/singer/by/1972----.htm
		Famine, Affluence and Morality
		Peter Singer
		https://en.wikipedia.org/wiki/Famine,_Affluence,_and_Morality
	</aside>
</section>
-->
<section data-background="img/volkswagen.jpg" data-background-size="cover">
	<section class="vw">
		<p>Volkswagen modified 11 million cars with software that would detect when they were being tested, and change performance to pass emissions tests.</p>
		<p class="fragment">As a result, these engines emitted nitrogen oxide pollutants up to 40 times above what is allowed by US emissions standards.</p>
		<aside class="notes">
			<p>Let's shift gears a little, and consider the Volkswagen emissions scandal. You may recall that VW added special software to millions of diesel cars, that would detect when their exhaust fumes were being checked by emissions regulators, and change performance to pass these tests.</p>
			<p>DOWN</p>
			<p>As a result, they managed to completely bypass emissions standards in the US, EU and elsewhere, for a period of about five years. Their workaround allowed them to emit up to 40 times more nitrogen oxide than what US emissions standards allow.</p>
			<p>DOWN</p>
		</aside>
	</section>
	<section class="vw">
		<p>Air pollution causes 40,000 early deaths per year in the UK alone.</p>
		<aside class="notes">
			<p>By some estimates, air pollution causes around 40,000 early deaths per year in the UK alone.</p>
			<p>It's pretty safe to assume that Volkswagen's technical hack is likely to result in several thousand premature deaths, plus thousands more cases of asthma and other lung disease.</p>
			<p>DOWN</p>
		</aside>
	</section>
	<section class="vw">
		<p>In August 2017, a former Volkswagen engineer was sentenced to three years in prison and fined $200,000 for his part in the scandal. Seven others are still facing charges.</p>
		<aside class="notes">
			<p>And as someone who recently started experiencing asthma symptoms for the first time since I was a child - probably because of London's air pollution - I take this pretty personally.</p>
			<p>So when I heard last year that one of the engineers at VW was imprisoned for his role in the scandal, I thought "GOOD".</p>
		</aside>
	</section>
</section>
<section data-background="img/grumpy.jpg" data-background-size="cover">
	<h1 class="grumpy">GOOD</h1>
	<aside class="notes">
		<p>But on the other hand, I've got to give credit where it's due: VW's "defeat device" is a brilliant technical hack. It's ingenious. I can imagine that the engineers who created it must have felt pretty proud of themselves at the time.</p>
	</aside>
</section>
<section data-background="img/baddies.gif" data-background-size="contain">
	<aside class="notes">
		<p>But at the same time, you also wonder why nobody spoke up at one of their internal meetings to say hey pals, do you think that maybe we're assholes?</p>
		<p>How did they get it so wrong? Are they just inherently bad people?</p>
	</aside>
</section>
<section data-background-color="#E5D6C7">
	<img src="img/bored-monkey.png" class="plain" />
	<aside class="notes">
		<p>Maybe it's because the monkey part of their brain was completely unable to deal with the complexity of the problem. You've got cars, and software hacks, and air pollution, and then decades later some people you don't know might die... It's all a bit much for the poor monkey to handle.</p>
	</aside>
</section>
<section data-background="img/forgetful.png" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>We established earlier that ethical reasoning involves an internal struggle for control. And a weird thing about humans is that we can sometimes actually ‚Äòforget‚Äô to act ethically, when we‚Äôre so focussed on achieving a goal that we forget to think about consequences of our actions.</p>
		<p>Or else we justify it to ourselves in ways that don‚Äôt stand up to scrutiny, but never stop to properly reflect.</p>
		<p>I‚Äôm sure we‚Äôve all done this at some point, I definitely have, and it‚Äôs led to some of my biggest screwups.</p>
	</aside>
</section>
<section data-background="img/move-fast-and-break-things.jpg" data-background-size="contain">
	<aside class="notes">
		<p>When you're just looking at a wall of code, it's very easy to forget about the human face of those who will be affected by your decisions. And unlike civil engineering, it's usually pretty easy to fix mistakes: Just roll out a patch or an update.</p>
		<p>In tech, we like to move fast and break things. But we don‚Äôt want to move fast into oncoming traffic and break people.</p>
	</aside>
</section>
<section>
	<section data-background="img/scandals.png" data-background-size="contain">
		<aside class="notes">
			<p>And I think this is at the heart of many of the biggest ethical lapses in tech today. Whether it's Facebook enabling fake news and Equifax with their criminally sloppy security, or JavaScript devs being too lazy to bother making their websites accessible for disabled people and keyboard users.</p>
			<p>I want to believe that the people making these decisions are doing so because they're not thinking hard enough about the consequences and the people affected by their actions.</p>
		</aside>
	</section>
	<section data-background="img/point.jpg" data-background-size="contain" data-background="white">
	</section>
</section>
<!-- 
TODO rewrite this to move each idea into its own section
<section>
	There are loads of different ways in which we face moral dilemmas in software engineering. 
	<h3 style="font-size: 1.4em">Moral decisions in web development</h3>
	<ul class="list">
		<li class="fragment">Security</li>
		<li class="fragment">Accessibility</li>
		<li class="fragment"><s>Fraternit√©</s> Choosing clients/employers</li>
		Dark patterns?
		Facebook? Fake news?
		Bias in algorithms. We can't be objective.
	</ul>
	<aside class="notes">
		<p>So far, I've covered a few examples of ethical dilemmas in software engineering. But what are some examples of ethical decisions that specifically affect front-end developers?</p>
		<p>DOWN</p>
		<p>Well, security, for a start. Caring about proper website security is a thankless task, where your manager or client usually won't notice if you've done your job right. But if you don't do it, you can run the risk of pulling an Equifax and completely screwing over your users.</p>
		Privacy e.g. GDPR etc
		<p>DOWN</p>
		<p>Similarly, you're making a moral decision in choosing how much effort to put into accessibility. In my experience, it's usually a choice between being lazy, and making access possible for disabled people.</p>
		<p>DOWN</p>
		<p>Deciding who to work for, and prioritising what to work on. Would you work for a tobacco company? A gun manufacturer? A gambling website, or a porn website?</p>
	</aside>
</section>
 -->
<section data-background="img/vonbraun.jpg" data-background-size="contain">
	<blockquote class="vonbraun">&lsquo;&ldquo;Once the rockets are up, who cares where they come down? That's not my department,&rdquo; says Wernher von Braun.&rsquo; &mdash; Tom Lehrer</blockquote>
	<aside class="notes">
		<p>However I also want to briefly address those who might say "Do I have to care about ethics? I'm just an engineer!"</p>
		<p>Like Mr Von Braun here, who knew he couldn't be in the forefront of rocket research in Germany if he didn't join all the right organisations ‚Äì and he didn't care what crimes to turn a blind eye to as long as he was allowed to play with his rockets.</p>
		<p>So let me be clear: Nobody is exempt from having to behave ethically. Ethics contaminates everything, and nobody is amoral. Whether you're building rockets or designing algorithms to help police identify gang members, you have a duty to consider how they might be used.</p>
	</aside>
	<!-- http://www.sciencemag.org/news/2018/02/artificial-intelligence-could-identify-gang-crimes-and-ignite-ethical-firestorm -->
</section>

<section data-background="img/reminders.png" data-background-size="cover" data-background-color="#F5F5F5">
	<aside class="notes">
		<p>With so many examples of ethically-compromised decision-making in tech, it's easy to get pessimistic.</p>
		<p>There's some good news, though: If it's easy for people to act unethically when they don't think about it, then the flip side to this, is that it follows that people tend to behave more ethically when you remind them to.</p>
		<p>And it can happen even when you do it in subtle ways.</p>
	</aside>
</section>
<section>
	<img src="img/eyes.png" class="plain" alt="Eyes emoji">
	<aside class="notes">
		<p>For instance, some researchers in Newcastle found that just hanging up posters of staring human eyes in a cafetaria was enough to significantly change people's behaviour, and it made people twice as likely to clean up after themselves.</p>
		<p>If just a poster of eyes can achieve that much, then imagine what else we could accomplish with a few well-placed reminders.</p>
	</aside>
</section>
<section>
	<h3>What can we do to<br> help developers make <br>ethical decisions?</h3>
	<aside class="notes">
		<p>So what can we do to help developers make ethical decisions?</p>
		<p>I think the best thing we can do is to use the power of reminders, and the fact that people tend to mimic the examples they see around them. So the real question is, what can we do to establish an organisational culture where people tend to act morally, and where there are lots of positive examples for us to emulate?</p>
	</aside>
</section>
<section>
	<section>
		<h2>Establish <br>codes of ethics</h2>
		<aside class="notes">
			<p>I mentioned before that many engineering industry bodies introduced formal codes of ethics in the early twentieth century.</p>
			<p>These came along with more legal regulations and barriers to entry, which I don't think is a good idea for us. But ethical codes are a great idea!</p>
			<p>These are a very good way to remind people to act ethically. Because basically, when you tell people ‚Äúhey don‚Äôt be a dick‚Äù, they will be less likely to be a dick.</p>
			<p>We already do this with codes of conduct at conferences and other events (including EMF Camp), and open-source Github repositories.</p>
		</aside>
	</section>
	<section>
		<blockquote class="code-of-ethics">
			<h3>Code of Ethics of the American Society of Civil Engineers</h3>
			<ol>
				<li>Engineers shall hold paramount the safety, health and welfare of the public and shall strive to comply with the principles of sustainable development in the performance of their professional duties.</li>
				<li>Engineers shall perform services only in areas of their competence.</li>
				<li>Engineers shall issue public statements only in an objective and truthful manner.</li>
				<li>Engineers shall act in professional matters for each employer or client as faithful agents or trustees, and shall avoid conflicts of interest.</li>
				<li>Engineers shall build their professional reputation on the merit of their services and shall not compete unfairly with others.</li>
				<li>Engineers shall act in such a manner as to uphold and enhance the honor, integrity, and dignity of the engineering profession and shall act with zero-tolerance for bribery, fraud, and corruption.</li>
				<li>Engineers shall continue their professional development throughout their careers, and shall provide opportunities for the professional development of those engineers under their supervision.</li>
			</ol>
		</blockquote>
		<aside class="notes">
			<p>We can do this at our organisations too. They don't have to be complicated, in fact, the simpler the better. The most important thing is to set appropriate expectations for ethical behaviour.</p>
		</aside>
	</section>
	<section>
		<h3>Existing codes of ethics:</h3>
		<ul>
			<li>Association for Computer Machinery</li>
			<!-- https://www.acm.org/code-of-ethics -->
			<li>Australian Computer Society</li>
			<li>British Computer Society</li>
			<li>Computer Ethics Institute</li>
		</ul>
		<p><small><a href="https://www.youtube.com/watch?v=GxKLgs8Vgsc">Patricia Realini - JSConf EU 2018</a></small></p>
	</section>
	<aside class="notes">
		<p>There are loads of other codes of ethics around.</p>
	</aside>
</section>
<section>
	<section>
		<h2>Establish <br>decision-making tools</h2>
		<aside class="notes">
			<p>I have a therapist friend who tells me that one of the ways they encourage ethical behaviour is to build ethics-checks into their processes.</p>
			<p>DOWN</p>
		</aside>
	</section>
	<section>
		<p><div class="emoji">üêí</div> Would you be happy for everyone to know<br> the decision you‚Äôve made?</p>
		<p><div class="emoji emoji--gap">ü§ñ</div> Do you think the consequences are acceptable?</p>
		<p><div class="emoji emoji--gap">ü§î</div> Would you recommend the same course<br> of action to others?</p>
		<aside class="notes">
			<p>So whenever they are trying to make a hard decision, they have these questions they can use,which are designed to trigger different types of emotional responses.</p>
			<p>So you've got the first one here, which I think is quite a monkey brain question, it's a good one for triggering emotional reactions like shame.</p>
			<p>So for example, if you're considerig being lazy about making your website accessible, imagine there's a disabled person sitting next to you, and think about whether you'd feel comfortable explaining your code choices to them.</p>
			<p>The second one seems designed to trigger more of a consequentialist response. It's maybe a bit of a rationalist, robot-brained approach?</p>
			<p>The last one reminds of Immanuel Kant's categorical imperative, that you should only do something if you'd be okay with it being a universal law. So that's cool too, if Kant's your thing.</p>
			<p>Overall I really like them: Good job psychotherapists!</p>
		</aside>
	</section>
</section>
<section>
	<section>
		<h2>Empower developers<br> to feel able to say no</h2>
		<aside class="notes">
			<p>A few years ago, I was working for a consultancy, who assigned me to a website project for a client I didn't approve of. But I got so invested in solving the technical aspects of the project, that I didn't stop to think about whether I was morally okay with working for this client, until I was already deeply invested. I moaned about the client to some colleagues, and they told me, if you didn't want to work for this client, that's fine, but you should have said something at the start of this project.</p>
			<p>It made me realise that it was okay to say no to client projects, but also that the appropriate time to do that is before you start work. The later you leave it, the harder it is to say no.</p>
			<p>The next time a dodgy client came along, I felt more comfortable expressing my concerns up-front, and we ended up turning down the client.</p>
		</aside>
	</section>
	<section>
		<h2>Communicate ethical principles</h2>
		<aside class="notes">
			<p>We can help encourage this sort of feedback by building these sorts of checks into our processes.</p>
			<p>One way you could do this is to add a checklist step in your p</p>
			<!--
				Think about what sort of ethical principles you'd choose for your own work, your team and your company.
				Get in the habit of considering consequences at the planning stage of a new project or feature. Maybe build it into your processes, include it in your documentation, or even introduce it as a step in a checklist?
				
				Discuss with your colleagues.

				Next, find ways to communicate these principles and establish them as part of the company culture. This could include adding it to your onboarding process for new starters. It could be company-wide emails, slack discussions, poster campaigns, or committees. Find whatever works for you and your team.

				Onboarding new starters
				Publicity campaigns
				Company-wide emails
				Codes of conduct
			-->
		</aside>
	</section>
	<section data-background="img/you-wouldnt.png" data-background-color="black" data-background-size="contain">
		<aside class="notes">
			<p>I tend to think of this as being like encouraging developers to submit bug reports and point out problems in your applications or processes. If everyone feels empowered to speak up, then you're all better off.</p>
		</aside>
	</section>
	<section data-background="img/Challenger_explosion.jpg" data-background-size="cover">
		<h2 style="text-shadow: -2px 4px 0 rgba(0,0,0,0.3), 0 0 40px rgba(0,0,0,0.5)">Whistleblowing</h2>
		<aside class="notes">
			<p>On that note: If you speak up about ethically dubious practices at your workplace and your employer doesn't listen, you may have a duty to report it to the authorities or otherwise make it public!</p>
			<p>A basic dilemma in engineering ethics is that an engineer has a duty to their client or employer, but an even greater duty to report a possible risk to others from a client or employer failing to follow the engineer's directions.</p>
			<p>A classic example of this is the Challenger Space Shuttle Disaster. NASA engineers raised warnings about faulty O-rings in the boosters, and the dangers posed by the low temperatures on the morning of the launch, but managers disregarded these warnings, and failed to adequately report these technical concerns to their superiors.</p>
			<p>It was later argued that in these circumstances, the engineers had had a duty to circumvent their managers and shout about the dangers until they were heard.</p>
		</aside>
	</section>
</section>
<section>
	<h2>Encourage developers <br>to meet users</h2>
	<aside class="notes">
		<p>Finally, we can help engineers to develop more empathy for their users, by encouraging them to meet them in person. A great way to do this is to get devs to sit in on user-testing sessions.</p>
		<p>A nice additional benefit of this is that empathy for your users helps you design better user-centred solutions, too. Win-win.</p>
	</aside>
</section>
<section>
	<h2>This is just the beginning.</h2>
	<aside class="notes">
		<p>These ideas are just a start, they won't fix everything.</p>
		<p>For example, it won't put a stop to the fact that a small handful of corporations own our digital lives and strip-mine them for profit.</p>
	</aside>
</section>
<section>
	<h4>A small handful of corporations own our digital lives and strip-mine them for profit</h4>
	<!-- https://www.theguardian.com/news/2018/may/03/why-silicon-valley-cant-fix-itself-tech-humanism -->
	<aside class="notes">
		
	</aside>
</section>
<section>
	<h1>Thanks!</h1>
	<p style="margin: 1em 0 0.8em">Don't forget to like and subscribe üëç</p>
	<a class="social" style="left:-1.85em" href="http://twitter.com/richardwestenra"> <i class="icon-twitter"></i> twitter.com/<b>richardwestenra</b></a>
	<a class="social" style="left:-1.85em" href="http://github.com/richardwestenra"> <i class="icon-github"></i> github.com/<b>richardwestenra</b></a>
	<a class="social" style="left:0em" href="mailto:richard@richardwestenra.com"> <i class="icon-email"></i> richard@<b>richardwestenra</b>.com</a>
	<a class="social" style="left:2.17em" href="http://richardwestenra.com"> <i class="icon-home"></i> <b>richardwestenra</b>.com</a>
	<!-- <a class="social" style="left:-2.8em" href="http://linkedin.com/in/richardwestenra"> <i class="icon-linkedin"></i> linkedin.com/in/<b>richardwestenra</b></a> -->
	<!-- <a class="social" style="left:-2.5em" href="http://facebook.com/richardwestenra"> <i class="icon-facebook"></i> facebook.com/<b>richardwestenra</b></a> -->
	<aside class="notes">
		If you have have any more suggestions that you think I've missed, please come talk to me afterwards, or get in touch online using one of my imaginative usernames.
	</aside>
</section>
<!-- <section>
	<img src="img/socrates.jpg" class="plain" alt="Socrates" width="550px">
	<h2>Questions?</h2>
	<aside class="notes">
		<p>I'm happy to try to answer any questions you might have!</p>
		<p>However you might find that, like Socrates here, I only know that I know nothing.</p>
	</aside>
</section> -->
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				history: true,
				showNotes: window.location.search.match(/showNotes/gi),
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
